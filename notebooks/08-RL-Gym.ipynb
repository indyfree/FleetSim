{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents &lt;br&gt;&lt;/br&gt;<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-and-Data-loading\" data-toc-modified-id=\"Imports-and-Data-loading-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports and Data loading</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Autoreload all package before excecuting a call\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morty/uni/evsim/env/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Could not seed environment <FleetEnv<evsim-v0>>\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 11)                187       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 11)                0         \n",
      "=================================================================\n",
      "Total params: 763\n",
      "Trainable params: 763\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 6700 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  8/100 [=>............................] - ETA: 7s - reward: 1.2329 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morty/uni/evsim/env/lib/python3.7/site-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 4s 35ms/step - reward: 0.6395\n",
      "Interval 2 (100 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: -35.8053\n",
      "Interval 3 (200 steps performed)\n",
      "100/100 [==============================] - 2s 19ms/step - reward: 0.0000e+00\n",
      "Interval 4 (300 steps performed)\n",
      "100/100 [==============================] - 2s 19ms/step - reward: -44.7366\n",
      "Interval 5 (400 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.5886\n",
      "Interval 6 (500 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: -134.8743\n",
      "Interval 7 (600 steps performed)\n",
      "100/100 [==============================] - 2s 21ms/step - reward: -163.0912\n",
      "Interval 8 (700 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: -107.5341\n",
      "Interval 9 (800 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.7136\n",
      "Interval 10 (900 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.6058\n",
      "Interval 11 (1000 steps performed)\n",
      "100/100 [==============================] - 2s 19ms/step - reward: 0.4607\n",
      "Interval 12 (1100 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.4017\n",
      "Interval 13 (1200 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.3709\n",
      "Interval 14 (1300 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.5918\n",
      "Interval 15 (1400 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.5405\n",
      "Interval 16 (1500 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.4543\n",
      "Interval 17 (1600 steps performed)\n",
      "100/100 [==============================] - 2s 22ms/step - reward: 0.4673\n",
      "Interval 18 (1700 steps performed)\n",
      "100/100 [==============================] - 2s 21ms/step - reward: 0.4974\n",
      "Interval 19 (1800 steps performed)\n",
      "100/100 [==============================] - 2s 21ms/step - reward: 0.5041\n",
      "Interval 20 (1900 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.5135\n",
      "Interval 21 (2000 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.4439\n",
      "Interval 22 (2100 steps performed)\n",
      "100/100 [==============================] - 2s 21ms/step - reward: 0.5677\n",
      "Interval 23 (2200 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.0663\n",
      "Interval 24 (2300 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.0000e+00\n",
      "Interval 25 (2400 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.0441\n",
      "Interval 26 (2500 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.4844\n",
      "Interval 27 (2600 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.4996\n",
      "Interval 28 (2700 steps performed)\n",
      "100/100 [==============================] - 2s 21ms/step - reward: 0.5240\n",
      "Interval 29 (2800 steps performed)\n",
      "100/100 [==============================] - 2s 21ms/step - reward: 0.4981\n",
      "Interval 30 (2900 steps performed)\n",
      "100/100 [==============================] - 2s 21ms/step - reward: 0.6130\n",
      "Interval 31 (3000 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.3388\n",
      "Interval 32 (3100 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.3131\n",
      "Interval 33 (3200 steps performed)\n",
      "100/100 [==============================] - 2s 23ms/step - reward: 0.1852\n",
      "Interval 34 (3300 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.0000e+00\n",
      "Interval 35 (3400 steps performed)\n",
      "100/100 [==============================] - 2s 22ms/step - reward: 0.0000e+00\n",
      "Interval 36 (3500 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.2971\n",
      "Interval 37 (3600 steps performed)\n",
      "100/100 [==============================] - 2s 17ms/step - reward: 0.3995\n",
      "Interval 38 (3700 steps performed)\n",
      "100/100 [==============================] - 2s 17ms/step - reward: 0.1964\n",
      "Interval 39 (3800 steps performed)\n",
      "100/100 [==============================] - 2s 18ms/step - reward: 0.3611\n",
      "Interval 40 (3900 steps performed)\n",
      "100/100 [==============================] - 2s 17ms/step - reward: 0.4249\n",
      "Interval 41 (4000 steps performed)\n",
      "100/100 [==============================] - 2s 18ms/step - reward: 0.6015\n",
      "Interval 42 (4100 steps performed)\n",
      "100/100 [==============================] - 2s 17ms/step - reward: 0.5274\n",
      "Interval 43 (4200 steps performed)\n",
      "100/100 [==============================] - 2s 18ms/step - reward: 0.4388\n",
      "Interval 44 (4300 steps performed)\n",
      "100/100 [==============================] - 2s 21ms/step - reward: 0.4687\n",
      "Interval 45 (4400 steps performed)\n",
      "100/100 [==============================] - 2s 20ms/step - reward: 0.4709\n",
      "Interval 46 (4500 steps performed)\n",
      "100/100 [==============================] - 2s 18ms/step - reward: 0.4750\n",
      "Interval 47 (4600 steps performed)\n",
      "100/100 [==============================] - 2s 17ms/step - reward: 0.4506\n",
      "Interval 48 (4700 steps performed)\n",
      "100/100 [==============================] - 2s 17ms/step - reward: 0.4722\n",
      "Interval 49 (4800 steps performed)\n",
      "100/100 [==============================] - 2s 16ms/step - reward: 0.5321\n",
      "Interval 50 (4900 steps performed)\n",
      "100/100 [==============================] - 2s 16ms/step - reward: 0.5271\n",
      "Interval 51 (5000 steps performed)\n",
      "100/100 [==============================] - 2s 16ms/step - reward: 0.3956\n",
      "Interval 52 (5100 steps performed)\n",
      "100/100 [==============================] - 2s 16ms/step - reward: -8.5268\n",
      "Interval 53 (5200 steps performed)\n",
      "100/100 [==============================] - 2s 16ms/step - reward: 0.4442\n",
      "Interval 54 (5300 steps performed)\n",
      "100/100 [==============================] - 2s 19ms/step - reward: 0.5717\n",
      "Interval 55 (5400 steps performed)\n",
      "100/100 [==============================] - 2s 19ms/step - reward: 0.4433\n",
      "Interval 56 (5500 steps performed)\n",
      "100/100 [==============================] - 2s 17ms/step - reward: 0.4058\n",
      "Interval 57 (5600 steps performed)\n",
      "100/100 [==============================] - 2s 18ms/step - reward: -17.5241\n",
      "Interval 58 (5700 steps performed)\n",
      "100/100 [==============================] - 2s 18ms/step - reward: 0.4147\n",
      "Interval 59 (5800 steps performed)\n",
      "100/100 [==============================] - 2s 18ms/step - reward: -53.6128\n",
      "Interval 60 (5900 steps performed)\n",
      "100/100 [==============================] - 2s 17ms/step - reward: -8.4627\n",
      "Interval 61 (6000 steps performed)\n",
      "100/100 [==============================] - 2s 17ms/step - reward: 0.4649\n",
      "Interval 62 (6100 steps performed)\n",
      "100/100 [==============================] - 2s 17ms/step - reward: 0.4300\n",
      "Interval 63 (6200 steps performed)\n",
      "100/100 [==============================] - 2s 17ms/step - reward: 0.5201\n",
      "Interval 64 (6300 steps performed)\n",
      "100/100 [==============================] - 2s 16ms/step - reward: 0.4827\n",
      "Interval 65 (6400 steps performed)\n",
      "100/100 [==============================] - 2s 17ms/step - reward: -53.3320\n",
      "1 episodes - episode_reward: -60470.324 [-60470.324, -60470.324] - loss: 32999.426 - mean_absolute_error: 80.580 - mean_q: 10.091\n",
      "\n",
      "Interval 66 (6500 steps performed)\n",
      "100/100 [==============================] - 2s 16ms/step - reward: 0.3343\n",
      "Interval 67 (6600 steps performed)\n",
      "100/100 [==============================] - 2s 19ms/step - reward: 0.0399\n",
      "done, took 128.664 seconds\n",
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 2421.272, steps: 6429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f573818a6d8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evsim\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "\n",
    "ENV_NAME = 'evsim-v0'\n",
    "\n",
    "\n",
    "# Get the environment and extract the number of actions.\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "# Set the prediction accuracy of simulation\n",
    "env.prediction_accuracy(10)\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())\n",
    "\n",
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "\n",
    "dqn.fit(env, nb_steps=6700, visualize=False, verbose=1, log_interval=100)\n",
    "\n",
    "# After training is done, we save the final weights.\n",
    "\n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "\n",
    "dqn.test(env, nb_episodes=1, visualize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evsim",
   "language": "python",
   "name": "evsim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents <br></br>",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
